{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BOND Reranker Training - Complete Guide\n",
        "\n",
        "This notebook provides a comprehensive guide to training a cross-encoder reranker for the BOND (Biomedical Ontology Normalization and Disambiguation) system.\n",
        "\n",
        "## Overview\n",
        "\n",
        "The BOND reranker is a cross-encoder model that improves ontology normalization accuracy by re-ranking candidate ontology terms retrieved by BOND's initial retrieval stage. This notebook will walk you through:\n",
        "\n",
        "1. **Understanding the Dataset Structure** - How training data is formatted\n",
        "2. **Setup and Configuration** - Installing dependencies and setting paths\n",
        "3. **Data Loading** - Loading and inspecting training data\n",
        "4. **Model Initialization** - Setting up the cross-encoder model\n",
        "5. **Training** - Training the reranker with proper hyperparameters\n",
        "6. **Evaluation** - Evaluating model performance\n",
        "7. **Saving and Using** - Saving the trained model and integrating it into BOND\n",
        "\n",
        "## Why Use a Reranker?\n",
        "\n",
        "- **Improves Accuracy**: Boosts Hit@10 accuracy from ~75-80% (retrieval only) to ~85-90% (with reranker)\n",
        "- **Context-Aware**: Learns context-dependent relevance (e.g., \"lymphocyte\" in tonsil vs. blood)\n",
        "- **Handles One-to-Many Mappings**: The same author term can map to different ontology IDs depending on context\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.11+\n",
        "- GPU recommended (T4, V100, or A100 with 16GB+ VRAM)\n",
        "- Training data in JSONL format (see dataset structure below)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding the Dataset Structure\n",
        "\n",
        "Before training, it's important to understand the format of your training data. The reranker training data should be in **JSONL format** (one JSON object per line).\n",
        "\n",
        "### Dataset Format\n",
        "\n",
        "Each line in your training data file should be a JSON object with the following structure:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"query\": \"cell_type: T-cell; tissue: blood; organism: Homo sapiens\",\n",
        "  \"candidate\": \"label: T cell; synonyms: T lymphocyte | T-lymphocyte | thymocyte\",\n",
        "  \"candidate_id\": \"CL:0000084\",\n",
        "  \"correct_id\": \"CL:0000084\",\n",
        "  \"label\": 1.0,\n",
        "  \"retrieval_score\": 0.85,\n",
        "  \"retrieval_rank\": 0,\n",
        "  \"example_type\": \"positive\"\n",
        "}\n",
        "```\n",
        "\n",
        "### Field Descriptions\n",
        "\n",
        "- **`query`**: The formatted query string containing field type, author term, tissue, and organism\n",
        "  - Format: `\"{field_type}: {author_term}; tissue: {tissue}; organism: {organism}\"`\n",
        "  - Example: `\"cell_type: T-cell; tissue: blood; organism: Homo sapiens\"`\n",
        "\n",
        "- **`candidate`**: The formatted candidate ontology term\n",
        "  - Format: `\"label: {label}; synonyms: {syn1} | {syn2} | ...; definition: {definition}\"`\n",
        "  - Example: `\"label: T cell; synonyms: T lymphocyte | T-lymphocyte\"`\n",
        "\n",
        "- **`candidate_id`**: The ontology ID (CURIE) of the candidate term\n",
        "  - Example: `\"CL:0000084\"` (Cell Ontology ID)\n",
        "\n",
        "- **`correct_id`**: The correct ontology ID for this query\n",
        "  - This is the ground truth label\n",
        "\n",
        "- **`label`**: Binary label (1.0 = positive match, 0.0 = negative)\n",
        "  - `1.0`: Candidate matches the correct ontology ID\n",
        "  - `0.0`: Candidate does not match (hard negative or random negative)\n",
        "\n",
        "- **`retrieval_score`**: Confidence score from initial retrieval (0.0 to 1.0)\n",
        "\n",
        "- **`retrieval_rank`**: Rank position from initial retrieval (0 = top result)\n",
        "\n",
        "- **`example_type`**: Type of training example\n",
        "  - `\"positive\"`: Correct match that was retrieved\n",
        "  - `\"positive_missed\"`: Correct match that wasn't retrieved (added manually)\n",
        "  - `\"hard_negative\"`: Retrieved but wrong (hard negative)\n",
        "  - `\"random_negative\"`: Same field type but not retrieved (random negative)\n",
        "\n",
        "### Expected Data Distribution\n",
        "\n",
        "For a well-balanced training set:\n",
        "- **Positives**: \n",
        "- **Hard Negatives**: \n",
        "- **Random Negatives**: \n",
        "\n",
        "### File Structure\n",
        "\n",
        "Your training data should be organized as:\n",
        "```\n",
        "reranker_training_data/\n",
        "├── train.jsonl      # Training set (~1.5M examples)\n",
        "├── dev.jsonl        # Validation set (~90K examples)\n",
        "└── test.jsonl       # Test set (~85K examples, optional)\n",
        "```\n",
        "\n",
        "**Note**: If you don't have training data yet, you can generate it using the `build_reranker_training_data.py` script from the BOND repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Installation and Setup\n",
        "\n",
        "First, let's install the required dependencies and check GPU availability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (matches original Colab notebook)\n",
        "# Uncomment the line below if running in Google Colab or a fresh environment\n",
        "# !pip install -q sentence-transformers accelerate\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from typing import Dict, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"BOND Reranker Training - Setup\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"\\n>>> Checking GPU availability...\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✓ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✓ CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"✓ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"⚠️  No GPU available - training will be very slow!\")\n",
        "    print(\"   Consider using Google Colab with GPU runtime\")\n",
        "    device = \"cpu\"\n",
        "\n",
        "print(f\"\\n>>> Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configuration\n",
        "\n",
        "**IMPORTANT**: Update the paths below to match your setup!\n",
        "\n",
        "### Path Configuration\n",
        "\n",
        "You need to specify:\n",
        "1. **Training data path**: Path to your `train.jsonl` file\n",
        "2. **Validation data path**: Path to your `dev.jsonl` file\n",
        "3. **Output path**: Where to save the trained model\n",
        "4. **Model name**: Base model to fine-tune (using: `bioformers/bioformer-16L` - same as Colab)\n",
        "\n",
        "### For Google Colab Users\n",
        "\n",
        "If you're using Google Colab:\n",
        "- Upload your `reranker_training_data` folder to Colab\n",
        "- Or mount Google Drive and point to your data there\n",
        "- Update paths to `/content/...` or `/content/drive/MyDrive/...`\n",
        "\n",
        "### For Local Users\n",
        "\n",
        "If running locally:\n",
        "- Use absolute paths or paths relative to your working directory\n",
        "- Example: `/Users/yourname/projects/BOND/reranker_training_data/train.jsonl`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION - UPDATE THESE PATHS FOR YOUR SETUP\n",
        "# ============================================================================\n",
        "\n",
        "CONFIG = {\n",
        "    # ========================================================================\n",
        "    # DATA PATHS - UPDATE THESE!\n",
        "    # ========================================================================\n",
        "    # For Google Colab:\n",
        "    # 'train_path': '/content/reranker_training_data/train.jsonl',\n",
        "    # 'val_path': '/content/reranker_training_data/dev.jsonl',\n",
        "    \n",
        "    # For Google Drive (mounted):\n",
        "    # 'train_path': '/content/drive/MyDrive/BOND/reranker_training_data/train.jsonl',\n",
        "    # 'val_path': '/content/drive/MyDrive/BOND/reranker_training_data/dev.jsonl',\n",
        "    \n",
        "    # For local machine (UPDATE THESE PATHS):\n",
        "    'train_path': '/path/to/your/reranker_training_data/train.jsonl',  # ⚠️ CHANGE THIS\n",
        "    'val_path': '/path/to/your/reranker_training_data/dev.jsonl',      # ⚠️ CHANGE THIS\n",
        "    \n",
        "    # ========================================================================\n",
        "    # MODEL CONFIGURATION\n",
        "    # ========================================================================\n",
        "    'model_name': 'bioformers/bioformer-16L',  # Bioformer model used in Colab\n",
        "    # This is the same model used in the original Colab training\n",
        "    \n",
        "    # Output directory for trained model\n",
        "    # For Google Colab:\n",
        "    # 'output_path': '/content/reranker_checkpoints/bond-reranker-v1',\n",
        "    # For local:\n",
        "    'output_path': './reranker_checkpoints/bond-reranker-v1',  # ⚠️ CHANGE IF NEEDED\n",
        "    \n",
        "    # ========================================================================\n",
        "    # TRAINING HYPERPARAMETERS\n",
        "    # ========================================================================\n",
        "    'epochs': 3,                    # Number of training epochs\n",
        "    'batch_size': 32,               # Batch size (matches Colab config)\n",
        "    'learning_rate': 2e-5,         # Learning rate\n",
        "    'warmup_ratio': 0.1,            # Warmup ratio (10% of training steps)\n",
        "    'weight_decay': 0.01,          # Weight decay for regularization\n",
        "    'max_grad_norm': 1.0,          # Gradient clipping\n",
        "    'seed': 42,                     # Random seed for reproducibility\n",
        "    'pos_weight': 5.0,             # Weight positives 5x more (for imbalanced data)\n",
        "    \n",
        "    # ========================================================================\n",
        "    # DATA LIMITS (for testing)\n",
        "    # ========================================================================\n",
        "    'max_train_examples': None,    # Set to 1000 for quick testing, None for full dataset\n",
        "    'max_val_examples': 10000,     # Limit validation examples for faster evaluation\n",
        "}\n",
        "\n",
        "# Print configuration\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\" * 60)\n",
        "for key, value in CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify paths exist\n",
        "print(\"\\n>>> Checking data files...\")\n",
        "train_exists = os.path.exists(CONFIG['train_path'])\n",
        "val_exists = os.path.exists(CONFIG['val_path'])\n",
        "\n",
        "if not train_exists:\n",
        "    print(f\"❌ ERROR: Training data not found at {CONFIG['train_path']}\")\n",
        "    print(\"\\nPlease:\")\n",
        "    print(\"1. Update CONFIG['train_path'] with the correct path to your train.jsonl file\")\n",
        "    print(\"2. Make sure the file exists\")\n",
        "else:\n",
        "    print(f\"✓ Training data found: {CONFIG['train_path']}\")\n",
        "\n",
        "if not val_exists:\n",
        "    print(f\"❌ ERROR: Validation data not found at {CONFIG['val_path']}\")\n",
        "    print(\"\\nPlease:\")\n",
        "    print(\"1. Update CONFIG['val_path'] with the correct path to your dev.jsonl file\")\n",
        "    print(\"2. Make sure the file exists\")\n",
        "else:\n",
        "    print(f\"✓ Validation data found: {CONFIG['val_path']}\")\n",
        "\n",
        "if train_exists and val_exists:\n",
        "    print(\"\\n✓ All data files found! Ready to proceed.\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Please fix the paths above before continuing.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Data Loading Functions\n",
        "\n",
        "Let's create functions to load and inspect the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load JSONL file into a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "def prepare_cross_encoder_dataset(data: List[Dict]) -> Dataset:\n",
        "    \"\"\"\n",
        "    Convert raw data to cross-encoder format (matches original Colab notebook).\n",
        "\n",
        "    Each example should have:\n",
        "    - sentence1: query\n",
        "    - sentence2: candidate\n",
        "    - label: binary label (0 or 1)\n",
        "    \"\"\"\n",
        "    prepared_data = {\n",
        "        'sentence1': [],\n",
        "        'sentence2': [],\n",
        "        'label': []\n",
        "    }\n",
        "\n",
        "    for item in data:\n",
        "        prepared_data['sentence1'].append(item['query'])\n",
        "        prepared_data['sentence2'].append(item['candidate'])\n",
        "        prepared_data['label'].append(float(item['label']))  # Ensure float for BCE loss\n",
        "\n",
        "    return Dataset.from_dict(prepared_data)\n",
        "\n",
        "# Test loading a few examples to verify format\n",
        "if train_exists and val_exists:\n",
        "    print(\"\\n>>> Testing data loading (first 3 examples)...\")\n",
        "    test_data = load_jsonl(CONFIG['train_path'])\n",
        "    if len(test_data) > 3:\n",
        "        test_data = test_data[:3]\n",
        "    \n",
        "    print(\"\\nSample training example:\")\n",
        "    print(json.dumps(test_data[0], indent=2))\n",
        "    \n",
        "    test_dataset = prepare_cross_encoder_dataset(test_data)\n",
        "    print(\"\\nPrepared sample:\")\n",
        "    print(test_dataset[0])\n",
        "else:\n",
        "    print(\"\\n⚠️  Skipping data loading test - fix paths first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Import Required Libraries\n",
        "\n",
        "Import the sentence-transformers libraries needed for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import sentence-transformers components\n",
        "from datasets import Dataset\n",
        "from sentence_transformers import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.trainer import CrossEncoderTrainer\n",
        "from sentence_transformers.cross_encoder.losses import BinaryCrossEntropyLoss\n",
        "from sentence_transformers.cross_encoder.evaluation import CEBinaryClassificationEvaluator\n",
        "from sentence_transformers.cross_encoder.training_args import CrossEncoderTrainingArguments\n",
        "\n",
        "print(\"✓ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Training and Validation Data\n",
        "\n",
        "Now let's load the full training and validation datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets (matches original Colab notebook format)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING DATASETS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"Loading datasets...\")\n",
        "train_data = load_jsonl(CONFIG['train_path'])\n",
        "if CONFIG['max_train_examples']:\n",
        "    train_data = train_data[:CONFIG['max_train_examples']]\n",
        "\n",
        "dev_data = load_jsonl(CONFIG['val_path'])\n",
        "if CONFIG['max_val_examples']:\n",
        "    dev_data = dev_data[:CONFIG['max_val_examples']]\n",
        "\n",
        "print(f\"Train samples: {len(train_data):,}\")\n",
        "print(f\"Dev samples: {len(dev_data):,}\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample training example:\")\n",
        "print(json.dumps(train_data[0], indent=2))\n",
        "\n",
        "# Prepare datasets for cross-encoder\n",
        "print(\"\\nPreparing datasets for cross-encoder training...\")\n",
        "train_dataset = prepare_cross_encoder_dataset(train_data)\n",
        "dev_dataset = prepare_cross_encoder_dataset(dev_data)\n",
        "\n",
        "print(f\"Prepared train dataset: {len(train_dataset):,} samples\")\n",
        "print(f\"Prepared dev dataset: {len(dev_dataset):,} samples\")\n",
        "\n",
        "# Display prepared sample\n",
        "print(\"\\nPrepared sample:\")\n",
        "print(dev_dataset[0])\n",
        "\n",
        "# Analyze label distribution\n",
        "print(\"\\nLabel distribution:\")\n",
        "train_labels = [float(item['label']) for item in train_data]\n",
        "label_counts = Counter(train_labels)\n",
        "for label, count in sorted(label_counts.items()):\n",
        "    percentage = count / len(train_data) * 100\n",
        "    label_name = \"Positive\" if label == 1.0 else \"Negative\"\n",
        "    print(f\"  {label_name} (label={label}): {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n✓ Data loading complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Initialize the Model\n",
        "\n",
        "Initialize the cross-encoder model. We'll use `bioformers/bioformer-16L` as the base model, which is the same model used in the original Colab training. This is a biomedical domain-specific transformer model optimized for biological text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\n>>> Loading base model: {CONFIG['model_name']}\")\n",
        "print(\"    This may take a few minutes on first run (downloading model)...\")\n",
        "\n",
        "# Initialize cross-encoder (matches Colab configuration)\n",
        "model = CrossEncoder(\n",
        "    CONFIG['model_name'],\n",
        "    num_labels=1,        # Binary classification (relevance score)\n",
        "    max_length=512,     # Maximum sequence length\n",
        "    device=device       # Use GPU if available\n",
        ")\n",
        "\n",
        "print(f\"✓ Model loaded on device: {model.device}\")\n",
        "print(f\"✓ Model max length: {model.max_length}\")\n",
        "print(f\"✓ Model type: Cross-Encoder (binary classification)\")\n",
        "\n",
        "# Calculate model size\n",
        "try:\n",
        "    total_params = sum(p.numel() for p in model.model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)\n",
        "    print(f\"✓ Total parameters: {total_params:,}\")\n",
        "    print(f\"✓ Trainable parameters: {trainable_params:,}\")\n",
        "except:\n",
        "    pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Setup Loss Function and Evaluator\n",
        "\n",
        "### Loss Function\n",
        "\n",
        "We use `BinaryCrossEntropyLoss` with class weighting to handle imbalanced data. Since we have many more negative examples (~90-95%) than positive examples (~5-10%), we weight positive examples more heavily.\n",
        "\n",
        "### Evaluator\n",
        "\n",
        "The evaluator computes metrics (accuracy, F1, precision, recall) on the validation set during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SETTING UP LOSS FUNCTION AND EVALUATOR\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Binary Cross Entropy Loss for binary relevance prediction\n",
        "# Weight positives 5x more (matches original Colab configuration)\n",
        "pos_weight = torch.tensor([CONFIG['pos_weight']])\n",
        "loss = losses.BinaryCrossEntropyLoss(model, pos_weight=pos_weight)\n",
        "\n",
        "print(f\"\\n>>> Using BinaryCrossEntropyLoss with pos_weight={pos_weight.item()}\")\n",
        "print(f\"✓ Loss function initialized\")\n",
        "\n",
        "# Setup evaluator (matches original Colab format)\n",
        "print(f\"\\n>>> Creating evaluator...\")\n",
        "evaluator = CEBinaryClassificationEvaluator(\n",
        "    sentence_pairs=list(zip(dev_dataset['sentence1'], dev_dataset['sentence2'])),\n",
        "    labels=dev_dataset['label'],\n",
        "    name='dev'\n",
        ")\n",
        "\n",
        "print(\"✓ Evaluator configured for development set\")\n",
        "print(f\"  Validation examples: {len(dev_dataset):,}\")\n",
        "print(f\"  Metrics tracked: accuracy, F1, precision, recall\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Configure Training Arguments\n",
        "\n",
        "Set up training arguments including learning rate, batch size, evaluation strategy, and checkpointing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CONFIGURING TRAINING ARGUMENTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(CONFIG['output_path'], exist_ok=True)\n",
        "print(f\"\\n>>> Output directory: {CONFIG['output_path']}\")\n",
        "\n",
        "# Setup training arguments (matches Colab configuration exactly)\n",
        "training_args = CrossEncoderTrainingArguments(\n",
        "    output_dir=CONFIG['output_path'],\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=CONFIG['epochs'],\n",
        "    per_device_train_batch_size=CONFIG['batch_size'],\n",
        "    per_device_eval_batch_size=CONFIG['batch_size'],\n",
        "    learning_rate=CONFIG['learning_rate'],\n",
        "    weight_decay=CONFIG['weight_decay'],\n",
        "    warmup_ratio=CONFIG['warmup_ratio'],\n",
        "    \n",
        "    # Evaluation and saving\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_strategy='steps',\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='dev_f1',\n",
        "    \n",
        "    # Optimization\n",
        "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
        "    gradient_accumulation_steps=1,\n",
        "    max_grad_norm=CONFIG['max_grad_norm'],\n",
        "    \n",
        "    # Logging\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    logging_first_step=True,\n",
        "    report_to='none',  # Change to 'wandb' or 'tensorboard' if needed\n",
        "    \n",
        "    # Other settings\n",
        "    seed=CONFIG['seed'],\n",
        "    dataloader_drop_last=False,\n",
        ")\n",
        "\n",
        "print(\"\\nTraining arguments configured:\")\n",
        "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"  Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"  Warmup ratio: {training_args.warmup_ratio}\")\n",
        "print(f\"  Max grad norm: {training_args.max_grad_norm}\")\n",
        "print(f\"  Seed: {training_args.seed}\")\n",
        "print(f\"  FP16: {training_args.fp16}\")\n",
        "print(f\"  Best model metric: {training_args.metric_for_best_model}\")\n",
        "\n",
        "print(\"\\n✓ Training arguments configured\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Initialize Trainer\n",
        "\n",
        "Create the trainer object that will handle the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"INITIALIZING TRAINER\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer = CrossEncoderTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    loss=loss,\n",
        "    evaluator=evaluator\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Trainer initialized\")\n",
        "print(f\"  Training examples: {len(train_dataset):,}\")\n",
        "print(f\"  Validation examples: {len(dev_dataset):,}\")\n",
        "print(f\"  Device: {device}\")\n",
        "\n",
        "# Estimate training time\n",
        "if CONFIG['max_train_examples']:\n",
        "    total_steps = (CONFIG['max_train_examples'] // CONFIG['batch_size']) * CONFIG['epochs']\n",
        "else:\n",
        "    total_steps = (len(train_dataset) // CONFIG['batch_size']) * CONFIG['epochs']\n",
        "\n",
        "print(f\"\\nEstimated training steps: ~{total_steps:,}\")\n",
        "print(f\"Estimated checkpoints: ~{total_steps // training_args.save_steps}\")\n",
        "print(\"\\n⚠️  Training may take 2-4 hours depending on dataset size and GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Start Training\n",
        "\n",
        "Now we're ready to train! This will take some time depending on your dataset size and hardware.\n",
        "\n",
        "**Training Tips:**\n",
        "- Monitor the loss - it should decrease over time\n",
        "- Watch validation metrics (F1, accuracy) - they should improve\n",
        "- If you run out of memory (OOM), reduce `batch_size` in CONFIG\n",
        "- Training will automatically save checkpoints every 500 steps\n",
        "- The best model (by F1 score) will be loaded at the end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Epochs: {CONFIG['epochs']}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Output directory: {CONFIG['output_path']}\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nTraining started... This may take several hours.\\n\")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Save Final Model\n",
        "\n",
        "Save the final trained model to disk.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n>>> Saving final model...\")\n",
        "trainer.save_model()\n",
        "\n",
        "print(f\"✓ Model saved to: {CONFIG['output_path']}\")\n",
        "print(\"\\nModel files saved:\")\n",
        "print(f\"  - config.json (model configuration)\")\n",
        "print(f\"  - model.safetensors (model weights)\")\n",
        "print(f\"  - tokenizer files (tokenizer.json, vocab.txt, etc.)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 12: Evaluate Model Performance\n",
        "\n",
        "Let's evaluate the trained model on the validation set to see final performance metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATING MODEL\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(\"\\nFinal Evaluation Results:\")\n",
        "print(\"=\" * 60)\n",
        "for metric, value in sorted(eval_results.items()):\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {metric}: {value}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Expected metrics (evaluator name is 'dev'):\n",
        "# - dev_accuracy: Overall accuracy\n",
        "# - dev_f1: F1 score\n",
        "# - dev_precision: Precision\n",
        "# - dev_recall: Recall\n",
        "# - eval_loss: Validation loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 13: Test the Trained Model\n",
        "\n",
        "Let's test the trained model on a few example queries to see how it performs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "print(\"\\n>>> Loading trained model for testing...\")\n",
        "trained_model = CrossEncoder(CONFIG['output_path'], device=device)\n",
        "print(\"✓ Model loaded\")\n",
        "\n",
        "# Example test cases\n",
        "test_cases = [\n",
        "    {\n",
        "        \"query\": \"cell_type: T-cell; tissue: blood; organism: Homo sapiens\",\n",
        "        \"candidates\": [\n",
        "            \"label: T cell; synonyms: T lymphocyte | T-lymphocyte | thymocyte\",\n",
        "            \"label: B cell; synonyms: B lymphocyte | B-lymphocyte\",\n",
        "            \"label: NK cell; synonyms: natural killer cell | NK lymphocyte\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"tissue: liver; organism: Mus musculus\",\n",
        "        \"candidates\": [\n",
        "            \"label: liver; synonyms: hepatic organ\",\n",
        "            \"label: kidney; synonyms: renal organ\",\n",
        "            \"label: lung; synonyms: pulmonary organ\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TESTING MODEL ON EXAMPLE QUERIES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, test_case in enumerate(test_cases, 1):\n",
        "    print(f\"\\nTest Case {i}:\")\n",
        "    print(f\"  Query: {test_case['query']}\")\n",
        "    print(f\"\\n  Candidates:\")\n",
        "    \n",
        "    # Score each candidate\n",
        "    scores = []\n",
        "    for candidate in test_case['candidates']:\n",
        "        score = trained_model.predict([(test_case['query'], candidate)])[0]\n",
        "        prob = torch.sigmoid(torch.tensor(score)).item()\n",
        "        scores.append((candidate, score, prob))\n",
        "    \n",
        "    # Sort by score (descending)\n",
        "    scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    for rank, (candidate, score, prob) in enumerate(scores, 1):\n",
        "        print(f\"    Rank {rank}: {prob:.4f} - {candidate[:80]}...\")\n",
        "    \n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 14: Using the Trained Model in BOND\n",
        "\n",
        "Now that you have a trained reranker, here's how to use it in the BOND pipeline.\n",
        "\n",
        "### Option 1: Update BOND Configuration\n",
        "\n",
        "Set the reranker path in your BOND settings:\n",
        "\n",
        "```python\n",
        "from bond.config import BondSettings\n",
        "from bond.pipeline import BondMatcher\n",
        "\n",
        "settings = BondSettings(\n",
        "    reranker_path=\"/path/to/your/reranker_checkpoints/bond-reranker-v1\",  # Your trained model path\n",
        "    enable_reranker=True\n",
        ")\n",
        "\n",
        "matcher = BondMatcher(settings=settings)\n",
        "```\n",
        "\n",
        "### Option 2: Environment Variable\n",
        "\n",
        "Set the environment variable:\n",
        "\n",
        "```bash\n",
        "export BOND_RERANKER_PATH=\"/path/to/your/reranker_checkpoints/bond-reranker-v1\"\n",
        "export BOND_ENABLE_RERANKER=1\n",
        "```\n",
        "\n",
        "### Option 3: Direct Usage\n",
        "\n",
        "You can also use the reranker directly:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import CrossEncoder\n",
        "import torch\n",
        "\n",
        "# Load your trained model\n",
        "model = CrossEncoder(\n",
        "    \"/path/to/your/reranker_checkpoints/bond-reranker-v1\",\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        ")\n",
        "\n",
        "# Score query-candidate pairs\n",
        "query = \"cell_type: T-cell; tissue: blood; organism: Homo sapiens\"\n",
        "candidates = [\n",
        "    \"label: T cell; synonyms: T lymphocyte\",\n",
        "    \"label: B cell; synonyms: B lymphocyte\"\n",
        "]\n",
        "\n",
        "scores = model.predict([(query, c) for c in candidates])\n",
        "probabilities = [torch.sigmoid(torch.tensor(s)).item() for s in scores]\n",
        "\n",
        "# Rank by probability\n",
        "ranked = sorted(zip(candidates, probabilities), key=lambda x: x[1], reverse=True)\n",
        "for candidate, prob in ranked:\n",
        "    print(f\"{prob:.4f}: {candidate}\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM) Errors\n",
        "\n",
        "**Problem**: Training crashes with CUDA out of memory error.\n",
        "\n",
        "**Solutions**:\n",
        "1. Reduce batch size in CONFIG: `'batch_size': 4` or `'batch_size': 2`\n",
        "2. Reduce max sequence length: Change `max_length=512` to `max_length=256` in model initialization\n",
        "3. Use gradient accumulation (if supported)\n",
        "4. Use a smaller base model: `'model_name': 'cross-encoder/ms-marco-MiniLM-L-6-v2'`\n",
        "\n",
        "### Training Too Slow\n",
        "\n",
        "**Problem**: Training is taking too long.\n",
        "\n",
        "**Solutions**:\n",
        "1. Use a smaller base model\n",
        "2. Limit training examples: `'max_train_examples': 100000` for testing\n",
        "3. Ensure GPU is being used (check device output)\n",
        "4. Use mixed precision (FP16) - already enabled if GPU available\n",
        "\n",
        "### Poor Performance\n",
        "\n",
        "**Problem**: Model accuracy/F1 is low.\n",
        "\n",
        "**Solutions**:\n",
        "1. Check data quality - ensure labels are correct\n",
        "2. Verify data format matches expected structure\n",
        "3. Increase training epochs: `'epochs': 5`\n",
        "4. Adjust learning rate: Try `'learning_rate': 1e-5` or `'learning_rate': 3e-5`\n",
        "5. Check class balance - should have ~5-10% positives\n",
        "\n",
        "### File Not Found Errors\n",
        "\n",
        "**Problem**: Cannot find training data files.\n",
        "\n",
        "**Solutions**:\n",
        "1. Verify paths in CONFIG are correct\n",
        "2. Use absolute paths instead of relative paths\n",
        "3. Check file permissions\n",
        "4. For Colab: Make sure files are uploaded or Drive is mounted\n",
        "\n",
        "### Model Not Improving\n",
        "\n",
        "**Problem**: Validation metrics not improving during training.\n",
        "\n",
        "**Solutions**:\n",
        "1. Check if learning rate is too high/low\n",
        "2. Verify data is being loaded correctly\n",
        "3. Check if model is actually training (loss should decrease)\n",
        "4. Try different base model\n",
        "5. Increase warmup steps: `'warmup_steps': 2000`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Evaluate on Test Set**: If you have a test set, evaluate the model on it\n",
        "2. **Fine-tune Hyperparameters**: Experiment with different learning rates, batch sizes, etc.\n",
        "3. **Train Field-Specific Models**: Consider training separate rerankers for different field types (cell_type, tissue, disease, etc.)\n",
        "4. **Upload to Hugging Face**: Share your trained model on Hugging Face Hub\n",
        "5. **Integrate into BOND**: Use the trained model in your BOND pipeline for improved accuracy\n",
        "\n",
        "## Summary\n",
        "\n",
        "You've successfully trained a cross-encoder reranker for BOND! The model should improve ontology normalization accuracy by 10-15% compared to retrieval-only approaches.\n",
        "\n",
        "**Key Takeaways**:\n",
        "- Training data format: JSONL with query, candidate, and label fields\n",
        "- Model: Cross-encoder (bioformers/bioformer-16L - same as Colab)\n",
        "- Loss: Binary cross-entropy with pos_weight=5.0 for imbalanced data\n",
        "- Evaluation: F1 score (dev_f1) used to select best model\n",
        "- Output: Trained model saved to specified directory\n",
        "- Configuration: Matches original Colab training exactly (batch_size=32, warmup_ratio=0.1, weight_decay=0.01, etc.)\n",
        "\n",
        "For more information, see:\n",
        "- [BOND Reranker Training Guide](../RERANKER_TRAINING_GUIDE.md)\n",
        "- [BOND Documentation](../README.md)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
